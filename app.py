import streamlit as st
import joblib
import numpy as np
from sklearn.linear_model import LogisticRegression

# --- CONFIGURACI√ìN DE P√ÅGINA ---
st.set_page_config(
    page_title="Detector de Depresi√≥n", 
    page_icon="üß†",
    layout="centered"
)

# --- FUNCI√ìN DE CARGA ---
@st.cache_resource
def cargar_modelo():
    # Busca el archivo en la misma carpeta donde est√° app.py
    nombre_archivo = 'modelo_depresion.pkl'
    try:
        data = joblib.load(nombre_archivo)
        return data
    except FileNotFoundError:
        return None

# --- INTERFAZ PRINCIPAL ---
def main():
    st.title("üß† Detector de Patrones Depresivos")
    st.markdown("""
        Este modelo analiza texto para identificar indicadores ling√º√≠sticos de depresi√≥n..
        
        *Nota: Esto es una herramienta de demostraci√≥n y NO sustituye un diagn√≥stico profesional.*
        """)
    # Cargar modelo
    pack = cargar_modelo()
    
    if pack is None:
        st.error(f"‚ùå No se encuentra el archivo 'modelo_depresion_final_rbf.pkl'.")
        st.warning("Aseg√∫rate de haber descargado el archivo .pkl de Colab y ponerlo en esta misma carpeta.")
        st.stop()

    modelo = pack['modelo']
    vectorizer = pack['vectorizer']

    # √Årea de texto
    st.subheader("Ingresa el texto a analizar:")
    texto_usuario = st.text_area("Comentario:", height=150, placeholder="Escribe aqu√≠ en ingl√©s...")

    if st.button("Analizar Sentimiento"):
        if not texto_usuario.strip():
            st.warning("El texto est√° vac√≠o.")
        else:
            with st.spinner("Procesando..."):
                try:
                    # 1. Limpieza
                    texto_truncado = " ".join(texto_usuario.split()[:25])
                    
                    # 2. Vectorizaci√≥n
                    texto_vec = vectorizer.transform([texto_truncado])
                    
                    # 3. Predicci√≥n
                    prediccion = modelo.predict(texto_vec)[0]
                    
                    # Intentamos sacar probabilidad si el modelo lo permite
                    try:
                        probs = modelo.predict_proba(texto_vec)[0]
                        confianza = probs[1] if prediccion == 1 else probs[0]
                    except:
                        confianza = 0.0 # Si no tiene probabilidad activada

                    st.divider()

                    if prediccion == 1:
                        st.error("‚ö†Ô∏è Resultado: POSIBLE DEPRESI√ìN")
                        if confianza > 0:
                            st.write(f"Confianza del modelo: **{confianza*100:.1f}%**")
                        st.info("El modelo detect√≥ palabras y estructuras asociadas a la clase 'Depresi√≥n'.")
                    else:
                        st.success("‚úÖ Resultado: NO DEPRESI√ìN")
                        if confianza > 0:
                            st.write(f"Confianza del modelo: **{confianza*100:.1f}%**")

                except Exception as e:
                    st.error(f"Error al procesar: {e}")
    st.markdown("""
    Grupo 1: Los "Rescatados" (Sutiles)

    Estas son las frases que un umbral de 0.50 hubiera ignorado, pero tu 0.35 deber√≠a atrapar.



    "I just want to stay in bed all day."

    Por qu√©: No dice "triste" ni "depresi√≥n". Solo implica falta de energ√≠a (anhedonia). El modelo deber√≠a marcarla como Depresi√≥n (1).

    "I don't know why I feel this way."

    Por qu√©: Es vago. Un modelo estricto lo ignorar√≠a. Tu modelo sensible deber√≠a sospechar.

    "It's getting harder to pretend I'm okay."

    Por qu√©: La palabra "harder" y "pretend" suman puntos, pero quiz√°s no llegaban a 0.5. Con 0.35, deber√≠a saltar la alerta.

    Grupo 2: Los "Falsos Positivos Esperados" (El costo a pagar)

    Estas frases NO son depresi√≥n cl√≠nica, son quejas normales. Pero como bajaste la vara, es muy probable que el modelo diga que S√ç son depresi√≥n. Esto es normal.



    "I am so stressed about the exam tomorrow."

    Predicci√≥n probable: Depresi√≥n (1) (Falsa alarma).

    Raz√≥n: "Stressed" tiene peso negativo. El modelo prefiere equivocarse y alertarte.

    "I hate rainy days, they make me lazy."

    Predicci√≥n probable: Depresi√≥n (1) (Falsa alarma).

    Raz√≥n: "Hate" y "lazy" son palabras negativas.

    "I am exhausted from the gym."

    Predicci√≥n probable: Depresi√≥n (1) (Falsa alarma).

    Raz√≥n: "Exhausted" es s√≠ntoma f√≠sico de depresi√≥n. El modelo SVM no entiende bien el contexto "gym".

    Grupo 3: Los "Falsos Negativos" (Si falla aqu√≠, baja m√°s el umbral)

    Incluso con 0.35, estas son dif√≠ciles. Si el modelo dice "Sano", es que el truncamiento o el vocabulario nos limitan.



    "I'm fine." (Dicho sarc√°sticamente, pero texto plano).

    Predicci√≥n: Sano (0). (El modelo no es adivino).

    "Whatever."

    Predicci√≥n: Sano (0). (Demasiado corto, poca se√±al).

    Grupo 4: La Prueba de Sanidad (Debe dar 0 s√≠ o s√≠)

    Si el modelo marca esto como depresi√≥n, bajamos demasiado el umbral.



    "I am very happy with my life."

    Predicci√≥n: Sano (0).

    "The weather is beautiful today."

    Predicci√≥n: Sano (0).
        """)

if __name__ == '__main__':
    main()